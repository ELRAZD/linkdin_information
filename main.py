import time
import csv
import re
import os
import pickle
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class Data_Mining():
    def __init__(self):
        self.path = r"C:\Users\Elyar\Desktop\test"  # Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ùˆ chromedriver
        self.driver_path = os.path.join(self.path, "chromedriver.exe")
        self.cookies_path = os.path.join(self.path, "cookies.pkl")
        self.driver = None

    def setup_driver(self):
        if not os.path.exists(self.driver_path):
            raise FileNotFoundError(f"chromedriver Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ Ø¯Ø±: {self.driver_path}")

        chrome_options = Options()
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_argument("--disable-infobars")
        chrome_options.add_argument("--disable-extensions")

        self.driver = webdriver.Chrome(service=Service(self.driver_path), options=chrome_options)

    def login(self):
        self.driver.get("https://www.linkedin.com/")

        if os.path.exists(self.cookies_path):
            print("ğŸ” Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡...")
            with open(self.cookies_path, "rb") as cookies_file:
                cookies = pickle.load(cookies_file)
                for cookie in cookies:
                    if 'expiry' in cookie:
                        del cookie['expiry']
                    self.driver.add_cookie(cookie)

            self.driver.get("https://www.linkedin.com/feed/")
            time.sleep(3)

            if "feed" in self.driver.current_url:
                print("âœ… Ù„Ø§Ú¯ÛŒÙ† Ø¨Ø§ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")
                return
            else:
                print("âš ï¸ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³ØªÙ†Ø¯ ÛŒØ§ Ù…Ù†Ù‚Ø¶ÛŒ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù„Ø§Ú¯ÛŒÙ† Ø¯Ø³ØªÛŒ.")

        self.driver.get("https://www.linkedin.com/login/")
        print("ğŸ”‘ Ù„Ø·ÙØ§Ù‹ Ø§ÛŒÙ…ÛŒÙ„ Ùˆ Ø±Ù…Ø² Ø¹Ø¨ÙˆØ± Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ Ùˆ Ù„Ø§Ú¯ÛŒÙ† Ø´ÙˆÛŒØ¯ (Ùˆ Ú©Ù¾Ú†Ø§ Ø±Ø§ Ø­Ù„ Ú©Ù†ÛŒØ¯ Ø§Ú¯Ø± Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯)...")

        try:
            WebDriverWait(self.driver, 180).until(
                EC.presence_of_element_located((By.ID, "global-nav-search"))
            )
            print("âœ… ÙˆØ±ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

            cookies = self.driver.get_cookies()
            with open(self.cookies_path, "wb") as cookies_file:
                pickle.dump(cookies, cookies_file)
            print("ğŸ’¾ Ú©ÙˆÚ©ÛŒâ€ŒÙ‡Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")

        except:
            print("â›” Ø²Ù…Ø§Ù† ÙˆØ±ÙˆØ¯ ØªÙ…Ø§Ù… Ø´Ø¯ ÛŒØ§ ÙˆØ±ÙˆØ¯ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯.")
            self.driver.quit()
            exit()

    def jobs(self, job_name: str, location: str):
        self.driver.get("https://www.linkedin.com/jobs/")
        time.sleep(5)

        job_name_input = self.driver.find_element(By.CSS_SELECTOR, 'input[aria-label="Search by title, skill, or company"]')
        job_name_input.clear()
        job_name_input.send_keys(job_name)
        time.sleep(1)

        location_input = self.driver.find_element(By.CSS_SELECTOR, 'input[aria-label="City, state, or zip code"]')
        location_input.clear()
        location_input.send_keys(location)
        time.sleep(1)
        location_input.send_keys(Keys.RETURN)
        time.sleep(5)

    def job_scraper(self, counts: int):
        def loop_count(counts: int):
            return (counts // 25) + (1 if counts % 25 != 0 else 0)

        def clean_salary(text: str):
            match = re.search(r"\$[\d.,K]+/?[a-zA-Z]*(?:\s*-\s*\$[\d.,K]+/?[a-zA-Z]*)?", text)
            return match.group() if match else "None"

        job_data = []

        for _ in range(loop_count(counts)):
            time.sleep(2)
            jobs = self.driver.find_elements(By.XPATH, "//li[@data-occludable-job-id]")

            for job in jobs:
                self.driver.execute_script("arguments[0].scrollIntoView();", job)
                try:
                    job_id = job.get_attribute("data-occludable-job-id")

                    job_title_element = job.find_element(By.XPATH, ".//a[contains(@class, 'job-card-container__link')]")
                    job_title = job_title_element.text.strip()

                    company_element = job.find_element(By.XPATH, ".//div[contains(@class, 'artdeco-entity-lockup__subtitle')]")
                    company_name = company_element.text.strip()

                    location = "None"
                    try:
                        location_element = job.find_element(By.XPATH, ".//div[contains(@class, 'artdeco-entity-lockup__caption')]//li")
                        location = location_element.text.strip()
                    except:
                        pass

                    salary = "None"
                    try:
                        salary_element = job.find_element(By.XPATH, ".//div[contains(@class, 'artdeco-entity-lockup__metadata')]//li")
                        salary = clean_salary(salary_element.text.strip())
                    except:
                        pass

                    job_data.append({
                        'ID': job_id,
                        'Link': f"https://www.linkedin.com/jobs/view/{job_id}/",
                        'Title': job_title,
                        'Company': company_name,
                        'Location': location,
                        'Salary': salary
                    })
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´ØºÙ„: {e}")
                    continue

            try:
                next_button_element = self.driver.find_element(By.XPATH, "//button[@aria-label='View next page']")
                if next_button_element.is_enabled():
                    next_button_element.click()
                else:
                    print("ğŸ”š Ø¯Ú©Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª. ØªÙˆÙ‚Ù.")
                    break
            except Exception as e:
                print(f"â›” Ù†ØªÙˆØ§Ù†Ø³Øª Ø¯Ú©Ù…Ù‡ Ø¨Ø¹Ø¯ÛŒ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯ ÛŒØ§ Ú©Ù„ÛŒÚ© Ú©Ù†Ø¯: {e}")
                break

        job_data_path = os.path.join(self.path, "job_data.csv")
        with open(job_data_path, "w", newline='', encoding='utf-8') as jobs_data:
            writer = csv.DictWriter(jobs_data, fieldnames=["ID", "Link", "Title", "Company", "Location", "Salary"])
            writer.writeheader()
            writer.writerows(job_data)

        print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(job_data)} Ø´ØºÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯. ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {job_data_path}")


if __name__ == "__main__":
    data = Data_Mining()
    data.setup_driver()
    data.login()
    data.jobs('java', 'germany')
    data.job_scraper(50)